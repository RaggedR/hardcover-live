#!/usr/bin/env python3
"""
Visualization of Recommendation System Performance

Compares all approaches tested:
- Pure collaborative filtering (original)
- Improved collaborative filtering
- Popularity baseline
- Hybrid models (various weight combinations)

Generates:
- Performance comparison table
- Improvement progression chart (text-based)
- Key insights summary
"""

print("="*80)
print("HARDCOVER RECOMMENDATION SYSTEM - PERFORMANCE COMPARISON")
print("="*80)

# Results from all experiments
results = {
    'approaches': [
        {
            'name': 'Pure Collaborative Filtering',
            'description': 'Matrix factorization, 5 features, Œª=5',
            'precision_at_10': 2.16,
            'test_accuracy': 94.69,
            'training_time_sec': 12,
            'data_signals': 25_132,
            'sparsity': 99.77,
            'key_issue': 'Learns global bias (95% likes), not individual preferences'
        },
        {
            'name': 'Improved Collaborative',
            'description': 'Filter users ‚â•20 ratings, 20 features, Œª=1.0, normalization',
            'precision_at_10': 2.45,
            'test_accuracy': 94.80,
            'training_time_sec': 15,
            'data_signals': 15_174,
            'sparsity': 97.58,
            'key_issue': 'Better filtering but still sparse, modest improvement'
        },
        {
            'name': 'Popularity Baseline',
            'description': 'Recommend books with most users (no training)',
            'precision_at_10': 7.56,
            'test_accuracy': 94.77,
            'training_time_sec': 0,
            'data_signals': 26_598,
            'sparsity': 95.75,
            'key_issue': 'No personalization, but proven effective'
        },
        {
            'name': 'Collaborative (Implicit Feedback)',
            'description': 'With want_to_read + currently_reading signals',
            'precision_at_10': 5.31,
            'test_accuracy': None,
            'training_time_sec': 15,
            'data_signals': 26_598,
            'sparsity': 95.75,
            'key_issue': 'Implicit feedback helps! 117% improvement over pure collab'
        },
        {
            'name': 'üèÜ HYBRID (50/50)',
            'description': '50% popularity + 50% collaborative + implicit feedback',
            'precision_at_10': 8.83,
            'test_accuracy': None,
            'training_time_sec': 15,
            'data_signals': 26_598,
            'sparsity': 95.75,
            'key_issue': 'BEST - Combines strengths of both approaches'
        },
    ]
}

# Print detailed comparison table
print("\n" + "="*80)
print("DETAILED PERFORMANCE COMPARISON")
print("="*80)
print()

print(f"{'Approach':<35} {'Precision@10':>12} {'Accuracy':>10} {'Time (s)':>10}")
print("-"*80)

for approach in results['approaches']:
    name = approach['name']
    precision = f"{approach['precision_at_10']:.2f}%"
    accuracy = f"{approach['test_accuracy']:.2f}%" if approach['test_accuracy'] else "N/A"
    time = f"{approach['training_time_sec']}"

    print(f"{name:<35} {precision:>12} {accuracy:>10} {time:>10}")

# Performance progression visualization
print("\n" + "="*80)
print("PERFORMANCE PROGRESSION (Precision@10)")
print("="*80)
print()

max_precision = max(a['precision_at_10'] for a in results['approaches'])

for approach in results['approaches']:
    name = approach['name']
    precision = approach['precision_at_10']

    # Create bar chart (text-based)
    bar_length = int((precision / max_precision) * 50)
    bar = '‚ñà' * bar_length

    print(f"{name:<35} {bar} {precision:.2f}%")

# Improvements summary
print("\n" + "="*80)
print("IMPROVEMENT PROGRESSION")
print("="*80)
print()

baseline_precision = results['approaches'][0]['precision_at_10']  # Pure collaborative

print(f"Starting point (Pure Collaborative): {baseline_precision:.2f}%")
print()

for i, approach in enumerate(results['approaches'][1:], 1):
    precision = approach['precision_at_10']
    improvement = ((precision - baseline_precision) / baseline_precision) * 100

    if improvement > 0:
        symbol = "‚Üë"
    elif improvement < 0:
        symbol = "‚Üì"
    else:
        symbol = "‚Üí"

    print(f"Step {i}: {approach['name']}")
    print(f"  Precision: {precision:.2f}% ({symbol} {improvement:+.1f}% vs baseline)")
    print(f"  Key: {approach['description']}")
    print()

# Key insights
print("="*80)
print("KEY INSIGHTS")
print("="*80)
print()

insights = [
    {
        'title': '1. Implicit Feedback is Critical',
        'details': [
            'Adding "want to read" + "currently reading" added 11,424 signals (75% more data)',
            'Collaborative filtering improved from 2.45% ‚Üí 5.31% (117% gain)',
            'Reduced sparsity from 99.77% ‚Üí 95.75%'
        ]
    },
    {
        'title': '2. Pure Collaborative Fails with Sparse Data',
        'details': [
            'With 99.77% sparsity, model learns "most people like most books"',
            'Only 5.8% probability separation between likes/dislikes',
            'Doesn\'t learn individual user preferences effectively'
        ]
    },
    {
        'title': '3. Popularity is Surprisingly Strong',
        'details': [
            'Simple heuristic: recommend popular books',
            '7.56% precision - 3x better than pure collaborative!',
            'Works because popular books (1984, Harry Potter) are genuinely good'
        ]
    },
    {
        'title': '4. Hybrid Beats Both Pure Approaches',
        'details': [
            '50/50 balance is optimal (tested 30/70, 40/60, 50/50, 60/40, 70/30)',
            'Popularity provides quality baseline',
            'Collaborative adds personalization',
            '8.83% precision - 17% better than popularity alone!'
        ]
    },
    {
        'title': '5. Data Filtering Matters',
        'details': [
            'Filter to users with ‚â•20 ratings (246/1000 kept)',
            'Filter to books with ‚â•5 users (2,547/45,203 kept)',
            'Better to serve 246 users well than 1000 users poorly'
        ]
    }
]

for insight in insights:
    print(f"\n{insight['title']}")
    print("-" * 80)
    for detail in insight['details']:
        print(f"  ‚Ä¢ {detail}")

# Final recommendations
print("\n" + "="*80)
print("RECOMMENDATIONS FOR PRODUCTION")
print("="*80)
print()

print("‚úì USE: recommend.py (Hybrid 50/50)")
print(f"  - Best performance: 8.83% precision@10")
print(f"  - 4x better than pure collaborative filtering")
print(f"  - 17% better than popularity baseline")
print(f"  - Includes implicit feedback")
print()

print("‚úì WHY IT WORKS:")
print(f"  - Popularity (50%): Ensures quality recommendations")
print(f"  - Collaborative (50%): Adds personalization")
print(f"  - Implicit feedback: 75% more training data")
print(f"  - Balanced ensemble: Neither component dominates")
print()

print("‚ö† LIMITATIONS:")
print(f"  - Works for 246 users with ‚â•20 ratings (25% of dataset)")
print(f"  - New users with <20 ratings will get pure popularity")
print(f"  - 8.83% means ~1 out of 10 recommendations is good")
print(f"  - Still room for improvement with more data or content features")
print()

print("üìà FUTURE IMPROVEMENTS:")
print(f"  - Add content features (genres, authors, book descriptions)")
print(f"  - Collect more implicit feedback (clicks, time spent, etc.)")
print(f"  - Encourage users to rate more books (target 50-100 per user)")
print(f"  - Try advanced models (LightFM, neural collaborative filtering)")

# Comparison to industry standards
print("\n" + "="*80)
print("CONTEXT: HOW DOES 8.83% COMPARE?")
print("="*80)
print()

comparisons = [
    ("Random recommendations", "~0.3%", "Hardcover is 29x better"),
    ("Naive baseline (always like)", "~95%", "High accuracy but useless for ranking"),
    ("Hardcover hybrid", "8.83%", "Our result"),
    ("Netflix Prize winner", "~10-15%", "Industry benchmark (but denser data)"),
    ("Amazon product recommendations", "~15-25%", "Commercial systems (much more data)"),
]

print(f"{'System':<35} {'Precision@10':>15} {'Notes':<30}")
print("-"*80)
for system, precision, notes in comparisons:
    marker = "üëà" if system == "Hardcover hybrid" else ""
    print(f"{system:<35} {precision:>15} {notes:<30} {marker}")

print()
print("Note: Direct comparison is difficult due to different datasets and metrics.")
print("8.83% is strong performance given the sparse data (95.75% sparse).")

print("\n" + "="*80)
print("‚úì Analysis complete!")
print("="*80)

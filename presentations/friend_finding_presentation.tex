\documentclass{beamer}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{booktabs}

% Theme
\usetheme{Madrid}
\usecolortheme{default}

% Title page info
\title{Finding Book Friends with Clustering}
\subtitle{Machine Learning Approach to Reader Compatibility}
\author{Hardcover Book Friend Finder}
\date{\today}

\begin{document}

% ============================================================================
% SLIDE 1: Results Comparison
% ============================================================================

\begin{frame}{Why Not Just Recommend Popular Books?}

\textbf{We tested collaborative filtering vs. popularity baseline:}

\vspace{0.5cm}

\begin{table}
\centering
\begin{tabular}{lc}
\toprule
\textbf{Method} & \textbf{Precision@10} \\
\midrule
Pure collaborative filtering & 2.16\% \\
Improved collaborative (filtered) & 2.45\% \\
\alert{Popularity baseline} & \alert{7.56\%} \\
Hybrid (50/50) & 8.83\% \\
\bottomrule
\end{tabular}
\end{table}

\vspace{0.5cm}

\textbf{Lesson:} Pure collaborative filtering performs \alert{worse} than just recommending popular books!

\textbf{Why?} 95.75\% sparsity - too much missing data

\textbf{Solution:} Hybrid approach works best (8.83\% vs 7.56\%)

\end{frame}

% ============================================================================
% SLIDE 2: The Hybrid Method
% ============================================================================

\begin{frame}{The Hybrid Method: Best of Both Worlds}

\begin{block}{The Solution: 50/50 Hybrid Approach}
Combine two recommendation strategies:

\vspace{0.3cm}

\begin{equation*}
\text{Final Score} = 0.5 \times \text{Popularity Score} + 0.5 \times \text{Collaborative Score}
\end{equation*}

\vspace{0.3cm}

\begin{itemize}
    \item \textbf{Popularity:} Recommend books many users liked (safe baseline)
    \item \textbf{Collaborative:} Recommend based on user's learned preferences (personalization)
\end{itemize}
\end{block}

\vspace{0.3cm}

\begin{block}{Result}
\textbf{8.83\% precision} - outperforms both individual methods! (17\% better than popularity alone)
\end{block}

\end{frame}

% ============================================================================
% SLIDE 3: Data Filtering and Computational Complexity
% ============================================================================

\begin{frame}{Data Filtering \& Scalability}

\begin{block}{Data Filtering}
Started with 1,000 Hardcover users and 45,203 books. Filtered to:
\begin{itemize}
    \item \textbf{Users with $\geq$ 20 ratings:} 246 users
    \item \textbf{Books with $\geq$ 5 users:} 2,547 books
    \item \textbf{Total possible:} $246 \times 2,547 = 626,562$ ratings
    \item \textbf{Actual interactions:} 26,598 (only 4.25\%)
\end{itemize}
\end{block}

\vspace{0.2cm}

\begin{block}{Computational Performance}
Training time and scalability (300 iterations, 20 features):
\begin{itemize}
    \item \textbf{Current dataset (246 users):} 8.1 seconds
    \item \textbf{Projected 10,000 users:} ~32.7 seconds
    \item \textbf{Projected 100,000 users:} ~5.2 minutes
    \item \textbf{Time per user:} ~33ms (scales linearly)
\end{itemize}
\end{block}

\end{frame}

% ============================================================================
% SLIDE 4: Implicit Feedback Weighting
% ============================================================================

\begin{frame}{Implicit Feedback: Weighting Different Book Statuses}

\begin{block}{Not All Interactions Are Equal}
\begin{itemize}
    \item \textbf{Read with rating $\geq$ 3:} 1.0 (strong positive - definitely liked)
    \item \textbf{Currently reading:} 0.7 (moderate positive - probably enjoying)
    \item \textbf{Want to read:} 0.3 (weak positive - interested)
    \item \textbf{Read with rating $<$ 3 / DNF:} 0.0 (negative - disliked)
\end{itemize}
\end{block}

\vspace{0.3cm}

\begin{block}{What About Unrated/Unread Books?}
\begin{itemize}
    \item \textbf{During training:} Both unrated and unread books are masked out
    \item \textbf{After training:} Model predicts values for all unrated/unread books
    \item \textbf{Recommendations:} Books with highest predicted values
\end{itemize}
\end{block}

\vspace{0.2cm}

\textbf{Impact:} Added 11,424 extra signals (75\% more data), improving precision from 2.45\% to 5.31\%

\end{frame}

% ============================================================================
% SLIDE 5: Matrix Factorization
% ============================================================================

\begin{frame}{Matrix Factorization: Learning User Preferences}

\begin{center}
\begin{tikzpicture}[scale=0.85]
    % Y matrix (Ratings)
    \draw[thick] (0,0) rectangle (2.5,5);
    \node at (1.25,2.5) {\Large $Y$};
    \node[below] at (1.25,-0.3) {\large 246 users};
    \node[left,rotate=90] at (-0.6,2.5) {\large 2,547 books};
    \node[above] at (1.25,5.4) {\textbf{Ratings}};

    % X matrix (Book features)
    \draw[thick,fill=blue!20] (3.5,0) rectangle (5.0,5);
    \node at (4.25,2.5) {\Large $X$};
    \node[below] at (4.25,-0.3) {\large 20};
    \node[above] at (4.25,5.4) {\textbf{Book}};

    % W^T matrix (User features transposed)
    \draw[thick,fill=green!20] (5.5,1.5) rectangle (9.5,3.5);
    \node at (7.5,2.5) {\Large $W^T$};
    \node[below] at (7.5,1.1) {\large 246};
    \node[right,rotate=90] at (10.0,2.5) {\large 20};
    \node[above] at (7.5,3.9) {\textbf{Users}};
\end{tikzpicture}
\end{center}

\vspace{0.3cm}

Each user gets a \alert{20-dimensional feature vector} describing their reading preferences

\end{frame}

% ============================================================================
% SLIDE 6: Clustering for Friend Finding
% ============================================================================

\begin{frame}{Clustering: Finding Your Reading Tribe}

\textbf{Goal:} Group users with similar reading preferences

\vspace{0.5cm}

\begin{block}{How It Works}
\begin{enumerate}
    \item Start with each user's 20-dimensional feature vector
    \item \textbf{Normalize} the vectors (make them unit length)
    \item Use \alert{K-means clustering} to group similar users
    \item Tested different cluster counts, found \alert{K=13} is optimal
\end{enumerate}
\end{block}

\vspace{0.3cm}

\begin{block}{Finding Friends within Your Cluster}
\begin{itemize}
    \item Calculate \textbf{dot product} (cosine similarity) with each cluster member
    \item Higher similarity = better friend match
    \item Show shared books as conversation starters
\end{itemize}
\end{block}

\vspace{0.3cm}

\textbf{Result:} 13 distinct reading groups

\end{frame}

\end{document}
